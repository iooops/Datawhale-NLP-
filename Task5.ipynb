{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://github.com/datawhalechina/team-learning-nlp/blob/master/NewsTextClassification/Task5%20%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB2.md\n",
    "# https://tianchi.aliyun.com/notebook-ai/detail?spm=5176.12586969.1002.18.6406111aIKCSLV&postId=118268\n",
    "\n",
    "# 基于深度学习的文本分类\n",
    "\n",
    "# 1. Word2Vec\n",
    "# 2. TextCNN，TextRNN\n",
    "# 3. HAN\n",
    "\n",
    "\n",
    "# Word2Vec\n",
    "\n",
    "# word2vec的主要思路：通过单词和上下文彼此预测，对应的两个算法分别为：\n",
    "# Skip-grams (SG)：预测上下文\n",
    "# Continuous Bag of Words (CBOW)：预测目标单词\n",
    "\n",
    "# 另外提出两种更加高效的训练方法：\n",
    "# Hierarchical softmax\n",
    "# Negative sampling\n",
    "\n",
    "# word2vec分为2部分：1. 建立模型 2. 通过模型获取嵌入词向量\n",
    "# input_word, skip_window => input得到outpu的概率分布\n",
    "# input, output的one-hot编码\n",
    "\n",
    "# Skip-grams的引入：\n",
    "# 目的：减小权重矩阵规模\n",
    "# 方法：\n",
    "# 1. 将常见的单词组合（word pairs）或者词组作为单个“words”来处理\n",
    "# 2. 对高频次单词进行抽样来减少训练样本的个数\n",
    "# 3. 对优化目标采用“negative sampling”方法，这样每个训练样本的训练只会更新一小部分的模型权重，从而降低计算负担\n",
    "\n",
    "# Hierarchical Softmax：结合霍夫曼树做softmax，可以代替从隐藏层到输出softmax层的映射，减少softmax概率的计算量。\n",
    "\n",
    "\n",
    "# TextCNN, TextRNN\n",
    "\n",
    "# TextCNN利用CNN（卷积神经网络）进行文本特征抽取，不同大小的卷积核分别抽取n-gram特征，\n",
    "# 卷积计算出的特征图经过MaxPooling保留最大的特征值，然后将拼接成一个向量作为文本的表示。\n",
    "\n",
    "# TextRNN利用RNN（循环神经网络）进行文本特征抽取，由于文本本身是一种序列，而LSTM天然适合建模序列数据。\n",
    "# TextRNN将句子中每个词的词向量依次输入到双向双层LSTM，分别将两个方向最后一个有效位置的隐藏层拼接成一个向量作为文本的表示。\n",
    "\n",
    "\n",
    "# HAN \n",
    "# - Hierarchical Attention Network for Document Classification\n",
    "# 基于层级注意力，在单词和句子级别分别编码并基于注意力获得文档的表示，然后经过Softmax进行分类。\n",
    "# 其中word encoder的作用是获得句子的表示，可以替换为上节提到的TextCNN和TextRNN，也可以替换为下节中的BERT。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 本章作业\n",
    "# 尝试通过Word2Vec训练词向量\n",
    "# 尝试使用TextCNN、TextRNN完成文本表示\n",
    "# 尝试使用HAN进行文本分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10991ac30>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import logging\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)-15s %(levelname)s: %(message)s')\n",
    "\n",
    "# set seed \n",
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-28 17:06:09,732 INFO: Fold lens [1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000, 1000]\n"
     ]
    }
   ],
   "source": [
    "# split data to 10 fold\n",
    "fold_num = 10\n",
    "data_file = './data/train_set.csv'\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def all_data2fold(fold_num, num=10000):\n",
    "    fold_data = []\n",
    "    f = pd.read_csv(data_file, sep='\\t', encoding='UTF-8')\n",
    "    texts = f['text'].tolist()[:num]\n",
    "    labels = f['label'].tolist()[:num]\n",
    "\n",
    "    total = len(labels)\n",
    "\n",
    "    index = list(range(total))\n",
    "    np.random.shuffle(index)\n",
    "\n",
    "    all_texts = []\n",
    "    all_labels = []\n",
    "    for i in index:\n",
    "        all_texts.append(texts[i])\n",
    "        all_labels.append(labels[i])\n",
    "\n",
    "    label2id = {}\n",
    "    for i in range(total):\n",
    "        label = str(all_labels[i])\n",
    "        if label not in label2id:\n",
    "            label2id[label] = [i]\n",
    "        else:\n",
    "            label2id[label].append(i)\n",
    "\n",
    "    all_index = [[] for _ in range(fold_num)]\n",
    "    for label, data in label2id.items():\n",
    "        # print(label, len(data))\n",
    "        batch_size = int(len(data) / fold_num)\n",
    "        other = len(data) - batch_size * fold_num\n",
    "        for i in range(fold_num):\n",
    "            cur_batch_size = batch_size + 1 if i < other else batch_size\n",
    "            # print(cur_batch_size)\n",
    "            batch_data = [data[i * batch_size + b] for b in range(cur_batch_size)]\n",
    "            all_index[i].extend(batch_data)\n",
    "\n",
    "    batch_size = int(total / fold_num)\n",
    "    other_texts = []\n",
    "    other_labels = []\n",
    "    other_num = 0\n",
    "    start = 0\n",
    "    for fold in range(fold_num):\n",
    "        num = len(all_index[fold])\n",
    "        texts = [all_texts[i] for i in all_index[fold]]\n",
    "        labels = [all_labels[i] for i in all_index[fold]]\n",
    "\n",
    "        if num > batch_size:\n",
    "            fold_texts = texts[:batch_size]\n",
    "            other_texts.extend(texts[batch_size:])\n",
    "            fold_labels = labels[:batch_size]\n",
    "            other_labels.extend(labels[batch_size:])\n",
    "            other_num += num - batch_size\n",
    "        elif num < batch_size:\n",
    "            end = start + batch_size - num\n",
    "            fold_texts = texts + other_texts[start: end]\n",
    "            fold_labels = labels + other_labels[start: end]\n",
    "            start = end\n",
    "        else:\n",
    "            fold_texts = texts\n",
    "            fold_labels = labels\n",
    "\n",
    "        assert batch_size == len(fold_labels)\n",
    "\n",
    "        # shuffle\n",
    "        index = list(range(batch_size))\n",
    "        np.random.shuffle(index)\n",
    "\n",
    "        shuffle_fold_texts = []\n",
    "        shuffle_fold_labels = []\n",
    "        for i in index:\n",
    "            shuffle_fold_texts.append(fold_texts[i])\n",
    "            shuffle_fold_labels.append(fold_labels[i])\n",
    "\n",
    "        data = {'label': shuffle_fold_labels, 'text': shuffle_fold_texts}\n",
    "        fold_data.append(data)\n",
    "\n",
    "    logging.info(\"Fold lens %s\", str([len(data['label']) for data in fold_data]))\n",
    "\n",
    "    return fold_data\n",
    "\n",
    "\n",
    "fold_data = all_data2fold(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-28 17:06:13,755 INFO: Total 9000 docs.\n"
     ]
    }
   ],
   "source": [
    "# build train data for word2vec\n",
    "fold_id = 9\n",
    "\n",
    "train_texts = []\n",
    "for i in range(0, fold_id):\n",
    "    data = fold_data[i]\n",
    "    train_texts.extend(data['text'])\n",
    "    \n",
    "logging.info('Total %d docs.' % len(train_texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-28 17:09:18,564 INFO: Start training...\n",
      "2020-07-28 17:09:20,430 INFO: collecting all words and their counts\n",
      "2020-07-28 17:09:20,439 INFO: PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2020-07-28 17:09:21,475 INFO: collected 5295 word types from a corpus of 8191447 raw words and 9000 sentences\n",
      "2020-07-28 17:09:21,476 INFO: Loading a fresh vocabulary\n",
      "2020-07-28 17:09:21,532 INFO: effective_min_count=5 retains 4335 unique words (81% of original 5295, drops 960)\n",
      "2020-07-28 17:09:21,533 INFO: effective_min_count=5 leaves 8189498 word corpus (99% of original 8191447, drops 1949)\n",
      "2020-07-28 17:09:21,553 INFO: deleting the raw counts dictionary of 5295 items\n",
      "2020-07-28 17:09:21,554 INFO: sample=0.001 downsamples 61 most-common words\n",
      "2020-07-28 17:09:21,554 INFO: downsampling leaves estimated 7070438 word corpus (86.3% of prior 8189498)\n",
      "2020-07-28 17:09:21,569 INFO: estimated required memory for 4335 words and 100 dimensions: 5635500 bytes\n",
      "2020-07-28 17:09:21,569 INFO: resetting layer weights\n",
      "2020-07-28 17:09:22,416 INFO: training model with 8 workers on 4335 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2020-07-28 17:09:23,423 INFO: EPOCH 1 - PROGRESS: at 25.19% examples, 1764562 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-28 17:09:24,425 INFO: EPOCH 1 - PROGRESS: at 48.97% examples, 1711600 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-28 17:09:25,437 INFO: EPOCH 1 - PROGRESS: at 73.74% examples, 1722804 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-28 17:09:26,457 INFO: EPOCH 1 - PROGRESS: at 93.06% examples, 1621144 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-28 17:09:26,766 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-28 17:09:26,776 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-28 17:09:26,778 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-28 17:09:26,779 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-28 17:09:26,787 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-28 17:09:26,791 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-28 17:09:26,792 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-28 17:09:26,796 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-28 17:09:26,797 INFO: EPOCH - 1 : training on 8191447 raw words (7022049 effective words) took 4.4s, 1604175 effective words/s\n",
      "2020-07-28 17:09:27,807 INFO: EPOCH 2 - PROGRESS: at 19.87% examples, 1369972 words/s, in_qsize 13, out_qsize 2\n",
      "2020-07-28 17:09:28,830 INFO: EPOCH 2 - PROGRESS: at 39.74% examples, 1378706 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-28 17:09:29,831 INFO: EPOCH 2 - PROGRESS: at 60.03% examples, 1390399 words/s, in_qsize 14, out_qsize 1\n",
      "2020-07-28 17:09:30,844 INFO: EPOCH 2 - PROGRESS: at 82.80% examples, 1438377 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-28 17:09:31,536 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-28 17:09:31,548 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-28 17:09:31,550 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-28 17:09:31,562 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-28 17:09:31,571 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-28 17:09:31,574 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-28 17:09:31,578 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-28 17:09:31,580 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-28 17:09:31,580 INFO: EPOCH - 2 : training on 8191447 raw words (7022205 effective words) took 4.8s, 1468853 effective words/s\n",
      "2020-07-28 17:09:32,585 INFO: EPOCH 3 - PROGRESS: at 22.37% examples, 1564461 words/s, in_qsize 13, out_qsize 2\n",
      "2020-07-28 17:09:33,591 INFO: EPOCH 3 - PROGRESS: at 45.73% examples, 1605243 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-28 17:09:34,593 INFO: EPOCH 3 - PROGRESS: at 68.42% examples, 1603143 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-28 17:09:35,610 INFO: EPOCH 3 - PROGRESS: at 88.62% examples, 1553437 words/s, in_qsize 14, out_qsize 1\n",
      "2020-07-28 17:09:36,180 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-28 17:09:36,181 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-28 17:09:36,189 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-28 17:09:36,190 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-28 17:09:36,202 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-28 17:09:36,203 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-28 17:09:36,208 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-28 17:09:36,218 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-28 17:09:36,219 INFO: EPOCH - 3 : training on 8191447 raw words (7022165 effective words) took 4.6s, 1515257 effective words/s\n",
      "2020-07-28 17:09:37,242 INFO: EPOCH 4 - PROGRESS: at 22.08% examples, 1514817 words/s, in_qsize 14, out_qsize 1\n",
      "2020-07-28 17:09:38,244 INFO: EPOCH 4 - PROGRESS: at 45.81% examples, 1602826 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-28 17:09:39,245 INFO: EPOCH 4 - PROGRESS: at 69.73% examples, 1632965 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-28 17:09:40,249 INFO: EPOCH 4 - PROGRESS: at 94.49% examples, 1654082 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-28 17:09:40,451 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-28 17:09:40,452 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-28 17:09:40,454 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-28 17:09:40,454 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-28 17:09:40,458 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-28 17:09:40,470 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-28 17:09:40,476 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-28 17:09:40,483 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-28 17:09:40,484 INFO: EPOCH - 4 : training on 8191447 raw words (7022374 effective words) took 4.3s, 1650453 effective words/s\n",
      "2020-07-28 17:09:41,490 INFO: EPOCH 5 - PROGRESS: at 22.60% examples, 1578729 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-28 17:09:42,491 INFO: EPOCH 5 - PROGRESS: at 46.24% examples, 1624105 words/s, in_qsize 14, out_qsize 1\n",
      "2020-07-28 17:09:43,493 INFO: EPOCH 5 - PROGRESS: at 69.32% examples, 1623678 words/s, in_qsize 15, out_qsize 0\n",
      "2020-07-28 17:09:44,495 INFO: EPOCH 5 - PROGRESS: at 92.82% examples, 1629617 words/s, in_qsize 14, out_qsize 1\n",
      "2020-07-28 17:09:44,782 INFO: worker thread finished; awaiting finish of 7 more threads\n",
      "2020-07-28 17:09:44,787 INFO: worker thread finished; awaiting finish of 6 more threads\n",
      "2020-07-28 17:09:44,794 INFO: worker thread finished; awaiting finish of 5 more threads\n",
      "2020-07-28 17:09:44,803 INFO: worker thread finished; awaiting finish of 4 more threads\n",
      "2020-07-28 17:09:44,819 INFO: worker thread finished; awaiting finish of 3 more threads\n",
      "2020-07-28 17:09:44,828 INFO: worker thread finished; awaiting finish of 2 more threads\n",
      "2020-07-28 17:09:44,832 INFO: worker thread finished; awaiting finish of 1 more threads\n",
      "2020-07-28 17:09:44,847 INFO: worker thread finished; awaiting finish of 0 more threads\n",
      "2020-07-28 17:09:44,848 INFO: EPOCH - 5 : training on 8191447 raw words (7021693 effective words) took 4.4s, 1610955 effective words/s\n",
      "2020-07-28 17:09:44,853 INFO: training on a 40957235 raw words (35110486 effective words) took 22.4s, 1564918 effective words/s\n",
      "2020-07-28 17:09:44,854 INFO: precomputing L2-norms of word weight vectors\n",
      "2020-07-28 17:09:44,861 INFO: saving Word2Vec object under ./word2vec.bin, separately None\n",
      "2020-07-28 17:09:44,862 INFO: not storing attribute vectors_norm\n",
      "2020-07-28 17:09:44,866 INFO: not storing attribute cum_table\n",
      "2020-07-28 17:09:44,922 INFO: saved ./word2vec.bin\n"
     ]
    }
   ],
   "source": [
    "logging.info('Start training...')\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "num_features = 100     # Word vector dimensionality\n",
    "num_workers = 8       # Number of threads to run in parallel\n",
    "\n",
    "train_texts = list(map(lambda x: list(x.split()), train_texts))\n",
    "model = Word2Vec(train_texts, workers=num_workers, size=num_features)\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# save model\n",
    "model.save(\"./word2vec.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-07-28 17:09:47,602 INFO: loading Word2Vec object from ./word2vec.bin\n",
      "2020-07-28 17:09:47,639 INFO: loading wv recursively from ./word2vec.bin.wv.* with mmap=None\n",
      "2020-07-28 17:09:47,639 INFO: setting ignored attribute vectors_norm to None\n",
      "2020-07-28 17:09:47,640 INFO: loading vocabulary recursively from ./word2vec.bin.vocabulary.* with mmap=None\n",
      "2020-07-28 17:09:47,641 INFO: loading trainables recursively from ./word2vec.bin.trainables.* with mmap=None\n",
      "2020-07-28 17:09:47,642 INFO: setting ignored attribute cum_table to None\n",
      "2020-07-28 17:09:47,642 INFO: loaded ./word2vec.bin\n",
      "2020-07-28 17:09:47,653 INFO: storing 4335x100 projection weights into ./word2vec.txt\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model = Word2Vec.load(\"./word2vec.bin\")\n",
    "\n",
    "# convert format\n",
    "model.wv.save_word2vec_format('./word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 尝试学习Word2Vec的用法\n",
    "# https://colab.research.google.com/drive/1xjQKSjl_SahNAAw32yA3S_DpjGK8uKTh#scrollTo=TpFwPu5-K-PT\n",
    "\n",
    "# 尝试学习用CNN和RNN做classification\n",
    "# https://colab.research.google.com/drive/1hlPfLGPhcSBe9UF9-_eJ4KEbNXtOTeMa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
